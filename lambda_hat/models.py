# lambda_hat/models.py
"""Modernized model definitions using Haiku"""

from typing import List, Optional

import haiku as hk
import jax
import jax.numpy as jnp


def infer_widths(
    in_dim: int,
    out_dim: int,
    depth: int,
    target_params: Optional[int],
    fallback_width: int = 128,
) -> List[int]:
    """Infer widths to hit target_params, or use fallback"""
    if target_params is None:
        return [fallback_width] * depth
    # For simplicity, use constant width h and solve approximately:
    # P(h) = (in_dim+1)h + (L-1)(h+1)h + (h+1)out_dim â‰ˆ target_params
    L = depth
    a = L - 1  # coefficient of h^2
    b = (in_dim + 1) + (L - 1) + out_dim  # coefficient of h
    if a == 0:
        h = max(1, (target_params - out_dim) // (in_dim + 1))
    else:
        import math

        disc = float(b) * float(b) + 4.0 * float(a) * float(target_params)
        h = int((-float(b) + math.sqrt(disc)) / (2.0 * float(a)))
        h = int(max(1, h))
    return [h] * L


class MLP(hk.Module):
    """Flexible MLP architecture using Haiku"""

    def __init__(
        self,
        widths: List[int],
        out_dim: int,
        activation: str = "relu",
        bias: bool = True,
        init: str = "he",
        skip: bool = False,
        residual_period: int = 2,
        layernorm: bool = False,
        name: Optional[str] = None,
    ):
        super().__init__(name=name)
        self.widths = widths
        self.out_dim = out_dim
        self.activation = activation
        self.bias = bias
        self.init = init
        self.skip = skip
        self.residual_period = residual_period
        self.layernorm = layernorm

    def _get_activation(self):
        """Get activation function"""
        activations = {
            "relu": jax.nn.relu,
            "tanh": jnp.tanh,
            "gelu": jax.nn.gelu,
            "identity": lambda x: x,
        }
        return activations[self.activation]

    def _get_initializer(self):
        """Get weight initializer"""
        if self.init == "he":
            return hk.initializers.VarianceScaling(2.0, "fan_in", "truncated_normal")
        elif self.init == "xavier":
            return hk.initializers.VarianceScaling(1.0, "fan_avg", "truncated_normal")
        elif self.init == "lecun":
            return hk.initializers.VarianceScaling(1.0, "fan_in", "truncated_normal")
        elif self.init == "orthogonal":
            return hk.initializers.Orthogonal()
        else:
            return hk.initializers.TruncatedNormal(stddev=1.0)

    def __call__(self, x):
        """Forward pass with optional residuals and layer norm"""
        act = self._get_activation()
        w_init = self._get_initializer()

        h = x

        # Hidden layers
        for i, width in enumerate(self.widths):
            linear = hk.Linear(width, with_bias=self.bias, w_init=w_init, name=f"layer_{i}")
            z = linear(h)

            if self.layernorm:
                z = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(z)

            h_new = act(z)

            # Skip connections with dimension compatibility check
            if (
                self.skip
                and (i % self.residual_period == self.residual_period - 1)
                and h.shape[-1] == h_new.shape[-1]
            ):
                h = h + h_new
            else:
                h = h_new

        # Output layer (always Xavier initialization for stability)
        output_init = hk.initializers.VarianceScaling(1.0, "fan_in", "truncated_normal")
        y = hk.Linear(self.out_dim, with_bias=self.bias, w_init=output_init, name="output")(h)

        return y


def build_mlp_forward_fn(
    in_dim: int,
    widths: List[int],
    out_dim: int,
    activation: str = "relu",
    bias: bool = True,
    init: str = "he",
    skip: bool = False,
    residual_period: int = 2,
    layernorm: bool = False,
):
    """Build a Haiku-transformed MLP forward function"""

    def forward_fn(x):
        mlp = MLP(
            widths=widths,
            out_dim=out_dim,
            activation=activation,
            bias=bias,
            init=init,
            skip=skip,
            residual_period=residual_period,
            layernorm=layernorm,
        )
        return mlp(x)

    # Transform to pure JAX function
    model = hk.transform(forward_fn)
    return model


def count_params(params):
    """Count total parameters in a Haiku parameter tree"""
    return sum(p.size for p in jax.tree_util.tree_leaves(params))
