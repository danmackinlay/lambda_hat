# Artifact System

This project uses a **three-layer content-addressed artifact system** for reproducible ML experiments. Artifacts are organized by experiments with immutable content-addressed storage and automatic deduplication.

**Note on spelling:** The codebase uses `artefacts/` (British spelling) as the default directory name, but this documentation uses "artifacts" (American spelling) for clarity.

## Overview

The artifact system has three layers:

1. **Store** (`artefacts/store/`): Content-addressed immutable objects
2. **Experiments** (`artefacts/experiments/`): Experiment runs with symlinks to store
3. **Scratch** (`artefacts/scratch/`): Ephemeral cache (safe to delete)

This design ensures:
- **Deduplication**: Identical artifacts stored once
- **Reproducibility**: Content addressing guarantees deterministic IDs
- **Isolation**: Experiments don't interfere with each other
- **Efficiency**: Parsl reuses existing artifacts automatically

---

## Directory Structure

```
artefacts/  (or $LAMBDA_HAT_HOME, defaults to current dir)
│
├── store/
│   └── objects/
│       └── sha256/
│           └── <2>/<2>/<remaining 60 hex digits>/
│               ├── payload/         # Actual artifact content
│               │   ├── data.npz
│               │   ├── params.eqx
│               │   └── ...
│               └── meta.json        # Type, schema, provenance
│
├── experiments/
│   └── <experiment_name>/           # e.g., "dev", "production"
│       ├── manifest.jsonl           # Index of all runs in this experiment
│       ├── targets/                 # Symlinks to store objects
│       │   └── <target_id> -> ../../../store/objects/sha256/.../
│       ├── runs/
│       │   └── <run_id>/           # Format: YYYYmmddTHHMMSSZ-<algo>[-<tag>]-<rand6>
│       │       ├── manifest.json    # Run metadata with URN references
│       │       ├── inputs/
│       │       │   └── target -> ../../targets/<target_id>
│       │       ├── artifacts/       # Symlinks to store outputs
│       │       │   └── <name> -> ../../../../store/objects/sha256/.../
│       │       ├── tb/              # TensorBoard logs
│       │       ├── logs/            # stdout/stderr
│       │       │   ├── stdout.log
│       │       │   └── stderr.log
│       │       ├── parsl/           # Parsl run_dir (if using Parsl)
│       │       └── scratch/         # Temp working directory
│       └── tb/                      # Aggregated TensorBoard symlinks
│           └── <run_id> -> ../runs/<run_id>/tb/
│
└── scratch/                         # Ephemeral cache
    └── <temp files>                 # Safe to delete anytime
```

---

## Content-Addressed Storage

### URN Format

Artifacts are referenced using URNs (Uniform Resource Names):

```
urn:lh:<type>:sha256:<hash>
```

**Components:**
- `lh`: Lambda-Hat namespace
- `<type>`: Artifact type (`target`, `fit`, etc.)
- `sha256`: Hash algorithm
- `<hash>`: 64-character hex digest

**Example:**
```
urn:lh:target:sha256:0bde3f9072cd8af1afbb0f485b5e2ad3d12ddad3273b8a37967eee50acce9cea
```

### Store Layout

Objects are stored at paths derived from their SHA256 hash:

```
store/objects/sha256/<2>/<2>/<remaining 60>/
```

**Example:**
Hash `0bde3f9072...` maps to:
```
store/objects/sha256/0b/de/3f9072cd8af1afbb0f485b5e2ad3d12ddad3273b8a37967eee50acce9cea/
```

### Object Structure

Each object contains:

```
<hash>/
├── payload/         # Actual content
│   ├── data.npz     # (target artifacts)
│   ├── params.npz
│   ├── diagnostics/ # (optional: target diagnostics, local builds only)
│   │   ├── target_train_test_loss.png
│   │   ├── target_pred_vs_teacher_train.png
│   │   └── target_pred_vs_teacher_test.png
│   └── ...
└── meta.json        # Metadata
```

**Note:** Target diagnostics (teacher comparison plots) are conditionally generated:
- **Local builds** (`--backend local `): Generated by default (`LAMBDA_HAT_SKIP_DIAGNOSTICS=0`)
- **Parsl workflows**: Skipped by default (`LAMBDA_HAT_SKIP_DIAGNOSTICS=1`) to keep workers lightweight
- Can be regenerated on-demand using `lambda-hat diagnose-target`

**meta.json format:**
```json
{
  "type": "target",
  "schema": "1",
  "hash_algorithm": "sha256",
  "hash_value": "0bde3f90...",
  "created_at": "2025-11-16T03:49:48Z",
  "provenance": {
    "host": "crumpitt",
    "user": "dan",
    "config_yaml": "...",
    "code_version": "fec84f2"
  }
}
```

---

## Experiments and Runs

### Experiment Organization

Experiments group related runs under a common name (e.g., `dev`, `production`, `paper_v1`).

**Set experiment:**
```bash
export LAMBDA_HAT_EXPERIMENT=my_experiment
# Or use --experiment flag:
uv run lambda-hat build --experiment my_experiment ...
```

### Run Directory Format

Run IDs follow the pattern:
```
YYYYmmddTHHMMSSZ-<algo>[-<tag>]-<rand6>
```

**Examples:**
- `20251116T034945Z-build_target-tgt_6f6eef-ebd938`
- `20251116T091234Z-hmc-chain0-a3f2d1`
- `20251116T103021Z-vi-mfa-7c4e89`

**Components:**
- Timestamp (UTC): `20251116T034945Z`
- Algorithm: `build_target`, `hmc`, `vi`, etc.
- Optional tag: Target ID, chain number, etc.
- Random suffix: 6-char hex for uniqueness

### Run Manifest

Each run has a `manifest.json` describing inputs, outputs, and metadata:

```json
{
  "schema": "1",
  "run_id": "20251116T034945Z-build_target-tgt_6f6eef9cbf7e-ebd938",
  "experiment": "dev",
  "algo": "build_target",
  "host": "crumpitt",
  "created": "2025-11-16T03:49:48Z",
  "phase": "build_target",
  "inputs": [
    {
      "role": "config",
      "path": "config/experiments.yaml"
    }
  ],
  "outputs": [
    {
      "role": "target",
      "urn": "urn:lh:target:sha256:0bde3f9072cd8af1afbb0f485b5e2ad3d12ddad3273b8a37967eee50acce9cea"
    }
  ]
}
```

### Experiment Manifest

The experiment-level `manifest.jsonl` indexes all runs:

```jsonl
{"run_id": "20251116T034945Z-build_target-tgt_6f6eef9cbf7e-ebd938", "algo": "build_target", "created": "2025-11-16T03:49:48Z"}
{"run_id": "20251116T091234Z-hmc-chain0-a3f2d1", "algo": "hmc", "created": "2025-11-16T09:12:34Z"}
```

---

## Analysis Workflow (Golden Path)

### Worker Output (Stage B: Sampling)

Workers **always** write only raw traces, regardless of execution mode:

```
artefacts/experiments/<experiment>/runs/<run_id>/
├── manifest.json         # Run metadata (sampler, hyperparams, timings, work metrics)
└── traces_raw.json       # Raw trace arrays as JSON (LLC + scalar stats only)
```

**Design principles:**
- No ArviZ, no matplotlib, no metrics computation in workers
- Keeps HPC/cluster workers lightweight and fast
- Parameter vector traces forbidden (validation enforced)
- Single canonical format: JSON-serialized numpy arrays

**Example `traces_raw.json`:**
```json
{
  "llc": [[...], [...]],           // (chains, draws)
  "cumulative_fge": [[...], [...]],
  "cumulative_time": [[...], [...]],
  "acceptance_rate": [[...], [...]]
}
```

### Controller Reconstruction (Stage C: Diagnostics)

The controller loads raw traces and calls `analyze_traces()` to generate analysis products:

```
artefacts/experiments/<experiment>/runs/<run_id>/
├── traces_raw.json       # Input: raw traces
├── manifest.json         # Input: metadata
├── trace.nc              # Output: ArviZ InferenceData (cached)
├── analysis.json         # Output: metrics (llc_mean, ESS, R-hat, etc.)
└── diagnostics/          # Output: plots
    ├── trace.png         # ArviZ trace plot
    ├── rank.png          # Rank plot (convergence check)
    ├── energy.png        # Energy plot (HMC/MCLMC)
    ├── llc_convergence_combined.png  # LLC vs FGEs/Time
    └── wnv.png           # Work-normalized variance (full mode only)
```

**Smart caching:**
- If `trace.nc` and `analysis.json` exist and are fresh (mtime >= traces_raw.json), reuse them
- Otherwise, reconstruct via `analyze_traces()` (golden path)
- Use `--force` flag to bypass cache and recompute

**Commands:**
```bash
# Diagnose single run
uv run lambda-hat diagnose --run-dir artifacts/experiments/dev/runs/<run_id>

# Diagnose all runs in experiment
uv run lambda-hat diagnose-experiment --experiment dev --mode light

# Sample with immediate diagnostics (convenience)
uv run lambda-hat sample --config config.yaml --target-id tgt_abc --diagnose
```

### Auto-Diagnosis in Workflows

The workflow aggregation stage automatically diagnoses runs missing `analysis.json`:

```python
# Pseudo-code for aggregation logic
for run_dir in experiment_runs:
    if not analysis.json.exists() or analysis.json is stale:
        diagnose_entry(run_dir, mode="light")  # Auto-diagnose on-demand
    metrics = load(analysis.json)
    aggregate(metrics)
```

This ensures `llc_runs.parquet` always contains complete metrics without manual intervention.

### Promotion (Stage D)

The promotion stage (optional, enabled with `--promote` flag) creates galleries of diagnostic plots:

```
artifacts/experiments/{experiment}/
└── runs/
    └── {timestamp}-parsl_llc-{id}/
        └── artifacts/
            └── promotion/
                ├── hmc.png                          # Newest trace plot per sampler
                ├── vi.png
                ├── sgld.png
                └── gallery_trace.md                 # HTML snippet for README
```

**How it works**:
1. Promotion searches `experiments/{exp}/runs/` for sampler runs (identified by `manifest.json`)
2. For each sampler, finds the newest run containing the requested plot (e.g., `trace.png`)
3. Copies plots to promotion directory with standardized names (`{sampler}.png`)
4. Generates HTML gallery snippet with metrics overlay

**Supported plots**:
- `trace.png` - ArviZ trace plots
- `llc_convergence_combined.png` - LLC vs FGEs/Time convergence

**Commands**:
```bash
# Run workflow with promotion
uv run lambda-hat workflow llc --backend local --promote

# Manual promotion (if needed)
uv run lambda-hat promote gallery \
  --runs-root artifacts/experiments/dev/runs \
  --samplers hmc,vi,sgld \
  --plot-name trace.png \
  --outdir artifacts/promotion
```

**Note**: Target diagnostics (teacher comparison plots) are NOT promoted. They already exist at the correct location: `artifacts/experiments/{exp}/targets/{id}/diagnostics/` and can be accessed directly.

---

## TensorBoard Integration

### Per-Run Logs

Each run can write TensorBoard logs to its `tb/` directory:

```
artefacts/experiments/dev/runs/<run_id>/tb/
└── events.out.tfevents...
```

### Aggregated View

The experiment-level `tb/` directory contains symlinks for multi-run viewing:

```
artefacts/experiments/dev/tb/
├── run1 -> ../runs/20251116T091234Z-hmc-chain0-a3f2d1/tb/
├── run2 -> ../runs/20251116T091245Z-hmc-chain1-b4e3f2/tb/
└── run3 -> ../runs/20251116T091256Z-hmc-chain2-c5f4a3/tb/
```

### Viewing TensorBoard

```bash
# Get path to experiment TensorBoard directory
uv run lambda-hat artifacts tb my_experiment

# Launch TensorBoard
tensorboard --logdir $(uv run lambda-hat artifacts tb my_experiment)
```

---

## Garbage Collection

The GC system uses reachability analysis to safely remove unreachable objects.

### How It Works

1. Start from experiment manifests (roots)
2. Follow URN references to find reachable objects
3. Mark unreachable objects older than TTL
4. Remove unreachable objects from store

### Usage

```bash
# Preview what would be deleted
uv run lambda-hat artifacts gc --dry-run

# Delete artifacts older than 30 days
uv run lambda-hat artifacts gc --ttl-days 30

# Aggressive cleanup (7 day TTL)
uv run lambda-hat artifacts gc --ttl-days 7
```

**Safety:**
- Only removes objects not referenced by any experiment
- Respects TTL (time-to-live) to protect recent unreferenced artifacts
- Dry-run mode for safe testing

---

## Environment Variables

### LAMBDA_HAT_HOME

Root directory for all artifacts (default: `artefacts/` in current directory).

```bash
export LAMBDA_HAT_HOME=/mnt/scratch/lambda_hat_artifacts
```

### LAMBDA_HAT_STORE

Override store location (default: `$LAMBDA_HAT_HOME/store`).

```bash
export LAMBDA_HAT_STORE=/mnt/shared/lambda_hat_store
```

### LAMBDA_HAT_EXPERIMENTS

Override experiments location (default: `$LAMBDA_HAT_HOME/experiments`).

```bash
export LAMBDA_HAT_EXPERIMENTS=$HOME/experiments
```

### LAMBDA_HAT_EXPERIMENT

Set default experiment name (overridden by `--experiment` flag).

```bash
export LAMBDA_HAT_EXPERIMENT=production
```

### LAMBDA_HAT_SCRATCH

Override scratch location (default: `$LAMBDA_HAT_HOME/scratch`).

```bash
export LAMBDA_HAT_SCRATCH=/tmp/lambda_hat_scratch
```

---

## Reproducibility

### Content Addressing Ensures Determinism

Same configuration → same URN → reused artifacts:

```bash
# First run: builds target
uv run lambda-hat build --config-yaml config/experiments.yaml --target-id tgt_abc123

# Second run: reuses existing target (instant)
uv run lambda-hat build --config-yaml config/experiments.yaml --target-id tgt_abc123
```

**Why it works:**
- Target ID is a SHA256 hash of the configuration
- Same config → same hash → same URN
- System checks store before building
- If URN exists, symlink to existing object

### Parsl Integration

Parsl workflows automatically skip existing artifacts:

```python
# In Parsl workflow
@python_app
def build_target_app(cfg_yaml, target_id, experiment):
    from lambda_hat.commands.build_cmd import build_entry
    return build_entry(cfg_yaml, target_id, experiment)

# Parsl checks if output exists
# If URN found in store, task completes instantly
target_future = build_target_app(config, target_id, experiment)
```

---

## Working with Artifacts (Python API)

### Initialize Artifact System

```python
from lambda_hat.artifacts import Paths, ArtifactStore, RunContext

# Get paths from environment
paths = Paths.from_env()

# Initialize store
store = ArtifactStore(paths.store)

# Create run context
with RunContext(
    experiment="my_experiment",
    algo="my_algorithm",
    paths=paths
) as run:
    print(f"Run ID: {run.run_id}")
    print(f"Run directory: {run.run_dir}")
    print(f"Scratch directory: {run.scratch}")
```

### Store an Artifact

```python
import numpy as np
from pathlib import Path

# Create artifact content
artifact_dir = run.scratch / "my_artifact"
artifact_dir.mkdir(parents=True)
(artifact_dir / "data.npz").write_bytes(
    np.savez_compressed(...).tobytes()
)

# Store in content-addressed store
urn = store.put(
    payload_dir=artifact_dir,
    artifact_type="my_type",
    metadata={"key": "value"}
)

print(f"Stored as: {urn}")
# Output: urn:lh:my_type:sha256:a1b2c3...
```

### Retrieve an Artifact

```python
# Get artifact by URN
payload_path = store.get(urn)
print(f"Artifact content at: {payload_path}")

# Read artifact
data = np.load(payload_path / "data.npz")
```

### List Artifacts

```python
# List all runs in an experiment
experiment_dir = paths.experiments / "my_experiment"
manifest_path = experiment_dir / "manifest.jsonl"

with manifest_path.open() as f:
    for line in f:
        if line.strip():
            record = json.loads(line)
            print(f"{record['run_id']}: {record['algo']}")
```

---

## CLI Reference

### List Artifacts

```bash
# List all experiments
uv run lambda-hat artifacts ls

# List runs in an experiment
uv run lambda-hat artifacts ls my_experiment
```

### TensorBoard

```bash
# Get TensorBoard directory for experiment
uv run lambda-hat artifacts tb my_experiment

# Launch TensorBoard
tensorboard --logdir $(uv run lambda-hat artifacts tb my_experiment)
```

### Garbage Collection

```bash
# Preview what would be deleted
uv run lambda-hat artifacts gc --dry-run

# Delete unreachable artifacts older than 30 days
uv run lambda-hat artifacts gc --ttl-days 30
```

---

## Storage Considerations

### Deduplication

Content addressing ensures identical artifacts are stored once:

```bash
# Train same target 5 times
for i in {1..5}; do
    uv run lambda-hat build --config-yaml config/experiments.yaml --target-id tgt_abc123
done

# Result: Only 1 copy in store (all runs symlink to same object)
# Disk usage: ~10 MB (not 50 MB)
```

### Disk Usage

Typical storage for one experiment:

```
artefacts/store/objects/sha256/.../
├── <target_hash>/payload/
│   ├── data.npz           # ~10 MB (shared across all runs)
│   └── params.eqx         # ~1 MB (shared across all runs)
├── <hmc_trace_hash>/payload/
│   └── trace.nc           # ~50 MB (one per sampler run)
└── <vi_trace_hash>/payload/
    └── trace.nc           # ~5 MB (VI typically smaller)

artefacts/experiments/my_experiment/
├── runs/                  # Symlinks only (~1 KB each)
└── tb/                    # Symlinks only (~1 KB each)
```

**Total for N targets × M samplers:**
- Targets: `N × ~11 MB` (deduplicated)
- Traces: `N × M × ~50 MB` (not deduplicated, content varies)
- Overhead: `~1 KB × (N + N×M)` (symlinks and manifests)

### Cleanup Strategy

1. **Keep active experiments:**
   - Runs referenced in experiment manifests are protected
   - Use GC with high TTL (30+ days)

2. **Clean old experiments:**
   - Remove experiment directory: `rm -rf artefacts/experiments/old_experiment`
   - Run GC to remove unreferenced objects: `uv run lambda-hat artifacts gc --ttl-days 7`

3. **Emergency cleanup:**
   - Scratch is always safe to delete: `rm -rf artefacts/scratch/*`
   - Store should only be cleaned via GC (protects referenced objects)

---

## See Also

- [Experiments Guide](./experiments.md) - Config system and experiments.yaml
- [Workflows](./workflows.md) - Parsl orchestration, sweeps, and artifact management
- [Configuration Reference](./config.md) - Complete YAML schema
- [CLI Reference](../CLAUDE.md) - Command-line interface
